{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18298de9",
   "metadata": {},
   "source": [
    "# Topic Modeling: Organizing Unlabeled CVs with LDA\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates **Topic Modeling** using **Latent Dirichlet Allocation (LDA)** to organize unlabeled CVs (resumes) by automatically discovering hidden topics. Unlike supervised classification, topic modeling works with completely unlabeled data, making it ideal for organizing large document collections without manual labeling. You'll learn how to apply LDA to discover topics, interpret results, and organize documents based on their dominant topics.\n",
    "\n",
    "> \"The best way to find a needle in a haystack is to organize the haystack first.\"\n",
    "\n",
    "**The Problem**: You have a folder full of CVs‚Äîunlabeled, unorganized. You need to find candidates for specific roles, but manually reading through hundreds of CVs is impossible.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand what Topic Modeling is and why it's useful for unsupervised document organization\n",
    "- Learn how LDA (Latent Dirichlet Allocation) discovers hidden topics in text collections\n",
    "- Apply LDA to organize unlabeled documents automatically\n",
    "- Interpret topic modeling results by examining top words and document-topic distributions\n",
    "- Organize documents into folders based on their dominant topics\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Introduction to Topic Modeling** - What it is and why it's useful\n",
    "2. **What is LDA?** - Understanding Latent Dirichlet Allocation\n",
    "3. **The Pipeline** - Complete workflow from data loading to organization\n",
    "4. **Step 1: Loading Data** - Reading CVs from JSON files\n",
    "5. **Step 2: Preprocessing** - Cleaning and preparing text\n",
    "6. **Step 3: Vectorization** - Converting text to document-term matrix\n",
    "7. **Step 4: Training LDA** - Discovering topics automatically\n",
    "8. **Step 5: Analyzing Results** - Interpreting discovered topics\n",
    "9. **Step 6: Organizing Documents** - Creating folders and organizing CVs by topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded0fed",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "**Topic Modeling** is an **unsupervised learning** task that discovers hidden topics in a collection of unlabeled documents. Unlike classification (which requires labeled data), topic modeling finds patterns automatically.\n",
    "\n",
    "**Example applications:**\n",
    "- **Organizing unlabeled documents**: Group CVs by field (AI/ML, Data Analysis, etc.) without manual labeling\n",
    "- **Understanding large text collections**: Discover what themes exist in news archives, research papers, or social media\n",
    "- **Content recommendation**: Find documents similar to a given document based on topic similarity\n",
    "\n",
    "**Why it's useful:**\n",
    "- No labels needed: works with completely unlabeled data\n",
    "- Interpretable: topics are defined by their top words, making them understandable\n",
    "- Scalable: can process large document collections\n",
    "- Flexible: number of topics can be adjusted based on the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f61c3f",
   "metadata": {},
   "source": [
    "## What is LDA?\n",
    "\n",
    "**Latent Dirichlet Allocation (LDA)** is a probabilistic model that discovers hidden topics in a collection of documents.\n",
    "\n",
    "**Key idea**: \n",
    "- Each document is a **mixture of topics** (e.g., 70% AI/ML, 20% Data Analysis, 10% Software Engineering)\n",
    "- Each topic is a **distribution over words** (e.g., Topic 1: 30% \"PyTorch\", 25% \"TensorFlow\", 20% \"NLP\"...)\n",
    "- LDA discovers these topics automatically by finding words that co-occur together\n",
    "\n",
    "**For our CVs**: LDA will discover topics like \"AI/ML\", \"Data Analysis\", \"Big Data\" by looking at which words appear together, then assign each CV to the most relevant topic(s).\n",
    "\n",
    "**Reference**: Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). [Latent dirichlet allocation](https://dl.acm.org/doi/10.5555/944919.944937). *Journal of machine Learning research*, 3(Jan), 993-1022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf82c0",
   "metadata": {},
   "source": [
    "![Left: BoW. Right: LDA](../assets/lda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7018f6c",
   "metadata": {},
   "source": [
    "## The Pipeline\n",
    "\n",
    "1. **Load CVs**: Read all JSON files from topic folders using glob patterns and extract structured fields\n",
    "2. **Preprocess**: Clean the text (remove URLs, emails, etc.)\n",
    "3. **Vectorize**: Convert text to document-term matrix (Bag of Words)\n",
    "4. **Train LDA**: Discover topics automatically\n",
    "5. **Analyze Results**: See what topics were found and which CVs belong to each\n",
    "6. **Organize**: Create folders and copy CVs based on their dominant topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff53c5d",
   "metadata": {},
   "source": [
    "## Step 1: Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e9332f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File > Open Folder > W5_NLP\n",
    "# (VS Code root should be at W5_NLP)\n",
    "# Then run: `uv sync`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9f2514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "if not os.path.exists('B5'):\n",
    "    !git clone https://github.com/AFAskar/B5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5dc104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c1aa196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34 CV files from 3 topic folders:\n",
      "  1. 07\n",
      "  2. 10\n",
      "  3. 11\n",
      "  4. 15\n",
      "  5. 17\n",
      "  6. 20\n",
      "  7. 21\n",
      "  8. 22\n",
      "  9. 26\n",
      "  10. 29\n",
      "  11. 30\n",
      "  12. 33\n",
      "  13. 34\n",
      "  14. 39\n",
      "  15. 40\n",
      "  16. 08\n",
      "  17. 09\n",
      "  18. 12\n",
      "  19. 16\n",
      "  20. 18\n",
      "  21. 19\n",
      "  22. 23\n",
      "  23. 24\n",
      "  24. 31\n",
      "  25. 32\n",
      "  26. 35\n",
      "  27. 36\n",
      "  28. 37\n",
      "  29. 38\n",
      "  30. 13\n",
      "  31. 14\n",
      "  32. 25\n",
      "  33. 27\n",
      "  34. 28\n",
      "\n",
      "Combined structured data into text for 34 CVs\n"
     ]
    }
   ],
   "source": [
    "# Load CVs from JSON files in all topic folders\n",
    "cv_dir = Path('B5/W5_NLP/M1/datasets/CVs')\n",
    "# Use glob pattern to find all JSON files in Topic_* subdirectories, excluding English versions\n",
    "cv_files = sorted([f for f in cv_dir.glob('Topic_*/*.json') if not f.name.endswith('_en.json')])\n",
    "\n",
    "# Load and extract structured data from JSON\n",
    "cvs_data = []\n",
    "cv_names = []\n",
    "cv_file_paths = []  # Store original file paths for later copying\n",
    "\n",
    "for file in cv_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        cvs_data.append(data)\n",
    "        cv_names.append(file.stem)\n",
    "        cv_file_paths.append(file)  # Store the full path\n",
    "\n",
    "print(f\"Loaded {len(cvs_data)} CV files from {len(set(f.parent.name for f in cv_files))} topic folders:\")\n",
    "for i, name in enumerate(cv_names, 1):\n",
    "    print(f\"  {i}. {name}\")\n",
    "\n",
    "# Combine structured fields into text for each CV\n",
    "def combine_cv_fields(cv_json):\n",
    "    \"\"\"Combine Heading, Skills, Projects, Experience, Education into a single text\"\"\"\n",
    "    parts = []\n",
    "    \n",
    "    # Add heading\n",
    "    if 'Heading' in cv_json:\n",
    "        parts.append(cv_json['Heading'])\n",
    "    \n",
    "    # Add skills (join list items)\n",
    "    if 'Skills' in cv_json:\n",
    "        skills_text = ' '.join(cv_json['Skills']) if isinstance(cv_json['Skills'], list) else cv_json['Skills']\n",
    "        parts.append(skills_text)\n",
    "    \n",
    "    # Add projects\n",
    "    if 'Projects' in cv_json:\n",
    "        projects_text = ' '.join(cv_json['Projects']) if isinstance(cv_json['Projects'], list) else cv_json['Projects']\n",
    "        parts.append(projects_text)\n",
    "    \n",
    "    # Add experience\n",
    "    if 'Experience' in cv_json:\n",
    "        exp_text = ' '.join(cv_json['Experience']) if isinstance(cv_json['Experience'], list) else cv_json['Experience']\n",
    "        parts.append(exp_text)\n",
    "    \n",
    "    # Add education\n",
    "    if 'Education' in cv_json:\n",
    "        edu_text = ' '.join(cv_json['Education']) if isinstance(cv_json['Education'], list) else cv_json['Education']\n",
    "        parts.append(edu_text)\n",
    "    \n",
    "    return ' '.join(parts)\n",
    "\n",
    "# Convert JSON data to text\n",
    "cvs = [combine_cv_fields(cv_data) for cv_data in cvs_data]\n",
    "print(f\"\\nCombined structured data into text for {len(cvs)} CVs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbef1b5",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc5c7bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 34 CVs\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text: remove URLs, emails, and normalize whitespace\"\"\"\n",
    "    # Remove emails and URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Keep only Arabic/English letters and numbers\n",
    "    text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Preprocess all CVs\n",
    "cvs_processed = [preprocess_text(cv) for cv in cvs]\n",
    "print(f\"Preprocessed {len(cvs_processed)} CVs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1f26b",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Data for LDA\n",
    "\n",
    "Convert text to a document-term matrix (same as Bag of Words from classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a63cfb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Term Matrix: 34 CVs √ó 662 words\n",
      "Sparsity: 86.7%\n"
     ]
    }
   ],
   "source": [
    "# Create document-term matrix\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=1000,  # Top 1000 words\n",
    "    min_df=2,           # Word must appear in at least 2 CVs\n",
    "    max_df=0.8          # Ignore words in >80% of CVs\n",
    ")\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform(cvs_processed)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Document-Term Matrix: {doc_term_matrix.shape[0]} CVs √ó {doc_term_matrix.shape[1]} words\")\n",
    "print(f\"Sparsity: {(1 - doc_term_matrix.nnz / (doc_term_matrix.shape[0] * doc_term_matrix.shape[1])) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29757a7c",
   "metadata": {},
   "source": [
    "## Step 4: Train LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "042a3137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA to discover 3 topics...\n",
      "‚úì Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model\n",
    "n_topics = 3  # Number of topics to discover\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    random_state=42,\n",
    "    max_iter=10,\n",
    "    learning_method='online'\n",
    ")\n",
    "\n",
    "print(f\"Training LDA to discover {n_topics} topics...\")\n",
    "lda.fit(doc_term_matrix)\n",
    "print(\"‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b719dc1",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results\n",
    "\n",
    "Let's see what topics LDA discovered and which words define each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58b72bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "  Top words: engineer, analyst, analytics, 2019, 2021, business, with, 10, bi, governance\n",
      "  Weights: ['25.418', '25.095', '21.468', '20.625', '19.749', '19.336', '18.298', '16.163', '16.113', '15.502']\n",
      "\n",
      "Topic 2:\n",
      "  Top words: ai, on, models, engineer, research, computer, model, 06, 08, engineering\n",
      "  Weights: ['32.898', '28.287', '22.209', '20.499', '20.153', '20.043', '19.232', '16.574', '16.569', '15.889']\n",
      "\n",
      "Topic 3:\n",
      "  Top words: engineer, spark, big, hadoop, on, aws, platform, 2021, 01, time\n",
      "  Weights: ['7.766', '7.080', '5.884', '5.601', '4.821', '4.686', '4.546', '4.334', '4.237', '4.199']\n"
     ]
    }
   ],
   "source": [
    "# Display top words for each topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    \"\"\"Display top words for each topic\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        top_weights = [topic[i] for i in top_words_idx]\n",
    "        \n",
    "        print(f\"\\nTopic {topic_idx + 1}:\")\n",
    "        print(\"  Top words:\", \", \".join(top_words))\n",
    "        print(\"  Weights:\", [f\"{w:.3f}\" for w in top_weights])\n",
    "\n",
    "display_topics(lda, feature_names, n_top_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66088b1",
   "metadata": {},
   "source": [
    "**Interpreting the topics**: Look at the top words for each topic. Can you guess what each topic represents? For example:\n",
    "- Topic with \"PyTorch\", \"TensorFlow\", \"NLP\" ‚Üí probably AI/ML\n",
    "- Topic with \"Tableau\", \"Power BI\", \"dashboard\" ‚Üí probably Data Analysis\n",
    "- Topic with \"Hadoop\", \"Spark\", \"Kafka\" ‚Üí probably Big Data\n",
    "\n",
    "Now let's see which CV belongs to which topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f168fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Assignment to Topics:\n",
      "============================================================\n",
      "\n",
      "Topic 1 (17 CVs):\n",
      "  - 15 (99.4%)\n",
      "  - 29 (99.4%)\n",
      "  - 30 (99.3%)\n",
      "  - 08 (99.4%)\n",
      "  - 09 (99.5%)\n",
      "  - 18 (99.4%)\n",
      "  - 19 (99.4%)\n",
      "  - 23 (75.0%)\n",
      "  - 24 (79.7%)\n",
      "  - 31 (99.2%)\n",
      "  - 35 (99.3%)\n",
      "  - 36 (99.3%)\n",
      "  - 37 (99.3%)\n",
      "  - 38 (99.4%)\n",
      "  - 14 (99.5%)\n",
      "  - 27 (99.4%)\n",
      "  - 28 (99.4%)\n",
      "\n",
      "Topic 2 (14 CVs):\n",
      "  - 07 (99.5%)\n",
      "  - 10 (99.4%)\n",
      "  - 11 (99.5%)\n",
      "  - 17 (99.4%)\n",
      "  - 20 (99.4%)\n",
      "  - 21 (99.4%)\n",
      "  - 22 (99.4%)\n",
      "  - 26 (99.4%)\n",
      "  - 33 (99.4%)\n",
      "  - 34 (99.4%)\n",
      "  - 39 (99.4%)\n",
      "  - 40 (99.4%)\n",
      "  - 13 (84.1%)\n",
      "  - 25 (99.3%)\n",
      "\n",
      "Topic 3 (3 CVs):\n",
      "  - 12 (99.5%)\n",
      "  - 16 (99.3%)\n",
      "  - 32 (99.4%)\n"
     ]
    }
   ],
   "source": [
    "# Get topic distribution for each CV\n",
    "doc_topic_dist = lda.transform(doc_term_matrix)\n",
    "\n",
    "# Find dominant topic for each CV\n",
    "dominant_topics = doc_topic_dist.argmax(axis=1)\n",
    "\n",
    "# Create a DataFrame to see results\n",
    "df_results = pd.DataFrame({\n",
    "    'CV': cv_names,\n",
    "    'Dominant Topic': dominant_topics + 1,\n",
    "    'Topic Probabilities': [dist for dist in doc_topic_dist]\n",
    "})\n",
    "\n",
    "# Show which CVs belong to which topic\n",
    "print(\"CV Assignment to Topics:\")\n",
    "print(\"=\" * 60)\n",
    "for topic_id in range(n_topics):\n",
    "    topic_cvs = df_results[df_results['Dominant Topic'] == topic_id + 1]\n",
    "    print(f\"\\nTopic {topic_id + 1} ({len(topic_cvs)} CVs):\")\n",
    "    for idx, row in topic_cvs.iterrows():\n",
    "        prob = row['Topic Probabilities'][topic_id]\n",
    "        print(f\"  - {row['CV']} ({prob:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086c24a",
   "metadata": {},
   "source": [
    "## Step 6: Organize CVs into Folders\n",
    "\n",
    "Now comes the practical part: **automatically organize CVs into folders** based on their dominant topic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8ee314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 07.json ‚Üí Topic_2/\n",
      "Copied 10.json ‚Üí Topic_2/\n",
      "Copied 11.json ‚Üí Topic_2/\n",
      "Copied 15.json ‚Üí Topic_1/\n",
      "Copied 17.json ‚Üí Topic_2/\n",
      "Copied 20.json ‚Üí Topic_2/\n",
      "Copied 21.json ‚Üí Topic_2/\n",
      "Copied 22.json ‚Üí Topic_2/\n",
      "Copied 26.json ‚Üí Topic_2/\n",
      "Copied 29.json ‚Üí Topic_1/\n",
      "Copied 30.json ‚Üí Topic_1/\n",
      "Copied 33.json ‚Üí Topic_2/\n",
      "Copied 34.json ‚Üí Topic_2/\n",
      "Copied 39.json ‚Üí Topic_2/\n",
      "Copied 40.json ‚Üí Topic_2/\n",
      "Copied 08.json ‚Üí Topic_1/\n",
      "Copied 09.json ‚Üí Topic_1/\n",
      "Copied 12.json ‚Üí Topic_3/\n",
      "Copied 16.json ‚Üí Topic_3/\n",
      "Copied 18.json ‚Üí Topic_1/\n",
      "Copied 19.json ‚Üí Topic_1/\n",
      "Copied 23.json ‚Üí Topic_1/\n",
      "Copied 24.json ‚Üí Topic_1/\n",
      "Copied 31.json ‚Üí Topic_1/\n",
      "Copied 32.json ‚Üí Topic_3/\n",
      "Copied 35.json ‚Üí Topic_1/\n",
      "Copied 36.json ‚Üí Topic_1/\n",
      "Copied 37.json ‚Üí Topic_1/\n",
      "Copied 38.json ‚Üí Topic_1/\n",
      "Copied 13.json ‚Üí Topic_2/\n",
      "Copied 14.json ‚Üí Topic_1/\n",
      "Copied 25.json ‚Üí Topic_2/\n",
      "Copied 27.json ‚Üí Topic_1/\n",
      "Copied 28.json ‚Üí Topic_1/\n",
      "\n",
      "‚úì Organization complete! CVs are now in: output/organized_cvs\n"
     ]
    }
   ],
   "source": [
    "# Create output directory structure\n",
    "output_dir = Path('output/organized_cvs')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a folder for each topic\n",
    "for topic_id in range(n_topics):\n",
    "    topic_dir = output_dir / f\"Topic_{topic_id + 1}\"\n",
    "    topic_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy each CV to its topic folder\n",
    "for idx, (cv_name, topic_id, source_file) in enumerate(zip(cv_names, dominant_topics, cv_file_paths)):\n",
    "    target_dir = output_dir / f\"Topic_{topic_id + 1}\"\n",
    "    target_file = target_dir / f\"{cv_name}.json\"\n",
    "    \n",
    "    shutil.copy2(source_file, target_file)\n",
    "    print(f\"Copied {cv_name}.json ‚Üí Topic_{topic_id + 1}/\")\n",
    "\n",
    "print(f\"\\n‚úì Organization complete! CVs are now in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5be03",
   "metadata": {},
   "source": [
    "### Verify the Organization\n",
    "\n",
    "Let's check what's in each folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "445a901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic_1/ (17 CVs):\n",
      "  - 08.json\n",
      "  - 09.json\n",
      "  - 14.json\n",
      "  - 15.json\n",
      "  - 18.json\n",
      "  - 19.json\n",
      "  - 23.json\n",
      "  - 24.json\n",
      "  - 27.json\n",
      "  - 28.json\n",
      "  - 29.json\n",
      "  - 30.json\n",
      "  - 31.json\n",
      "  - 35.json\n",
      "  - 36.json\n",
      "  - 37.json\n",
      "  - 38.json\n",
      "\n",
      "Topic_2/ (14 CVs):\n",
      "  - 07.json\n",
      "  - 10.json\n",
      "  - 11.json\n",
      "  - 13.json\n",
      "  - 17.json\n",
      "  - 20.json\n",
      "  - 21.json\n",
      "  - 22.json\n",
      "  - 25.json\n",
      "  - 26.json\n",
      "  - 33.json\n",
      "  - 34.json\n",
      "  - 39.json\n",
      "  - 40.json\n",
      "\n",
      "Topic_3/ (3 CVs):\n",
      "  - 12.json\n",
      "  - 16.json\n",
      "  - 32.json\n"
     ]
    }
   ],
   "source": [
    "# Show contents of each topic folder\n",
    "for topic_id in range(n_topics):\n",
    "    topic_dir = output_dir / f\"Topic_{topic_id + 1}\"\n",
    "    files = list(topic_dir.glob('*.json'))\n",
    "    print(f\"\\nTopic_{topic_id + 1}/ ({len(files)} CVs):\")\n",
    "    for f in sorted(files):\n",
    "        print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48759d",
   "metadata": {},
   "source": [
    "## **Student Exercise**: discover topics on a dataset of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffa83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import joblib\n",
    "cache_dir= 'cache_github'\n",
    "memory = joblib.Memory(cache_dir, verbose=0)\n",
    "@memory.cache()\n",
    "def getREADMEs(user:str):\n",
    "    repos_url = f\"https://api.github.com/users/{user}/repos\"\n",
    "    response = requests.get(repos_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching repos for user {user}: {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    repos = response.json()\n",
    "    readmes = {}\n",
    "    \n",
    "    for repo in repos:\n",
    "        \n",
    "        repo_name = repo['name']\n",
    "        if repo_name.casefold() == user.casefold():\n",
    "            continue\n",
    "        if repo.get('fork'):\n",
    "            continue\n",
    "        \n",
    "        readme_url = f\"https://api.github.com/repos/{user}/{repo_name}/readme\"\n",
    "        readme_response = requests.get(readme_url, headers={'Accept': 'application/vnd.github.v3.raw'})\n",
    "        \n",
    "        if readme_response.status_code == 200:\n",
    "            readmes[repo_name] = readme_response.text\n",
    "        else:\n",
    "            readme_url_md = f\"https://api.github.com/repos/{user}/{repo_name}/contents/README.md\"\n",
    "            readme_response_md = requests.get(readme_url_md, headers={'Accept': 'application/vnd.github.v3.raw'})\n",
    "            if readme_response_md.status_code == 200:\n",
    "                readmes[repo_name] = readme_response_md.text\n",
    "            else:\n",
    "                readmes[repo_name] = None  # No README found or error occurred\n",
    "    \n",
    "    return readmes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46913c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching repositories for user: AFAskar...\n",
      "‚úì Found 9 repositories with READMEs\n",
      "\n",
      "Repositories found:\n",
      "  1. aqar-scraper\n",
      "  2. AzureStreamerBot\n",
      "  3. AzureTTVChat\n",
      "  4. CarDealerShipAPI\n",
      "  5. Common-Utility-Scripts\n",
      "  6. CSVProfiler\n",
      "  7. DisNote-bot\n",
      "  8. Foley-Mixer\n",
      "  9. InfoSecProject\n"
     ]
    }
   ],
   "source": [
    "# Fetch READMEs from a GitHub user\n",
    "github_user = \"AFAskar\"  # Change this to any GitHub username you want to analyze\n",
    "\n",
    "print(f\"Fetching repositories for user: {github_user}...\")\n",
    "readmes_dict = getREADMEs(github_user)\n",
    "\n",
    "# Filter out repos without READMEs\n",
    "readmes_dict = {repo: text for repo, text in readmes_dict.items() if text is not None}\n",
    "\n",
    "print(f\"‚úì Found {len(readmes_dict)} repositories with READMEs\\n\")\n",
    "\n",
    "# Display repository names\n",
    "print(\"Repositories found:\")\n",
    "for i, repo_name in enumerate(readmes_dict.keys(), 1):\n",
    "    print(f\"  {i}. {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b72940dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Term Matrix: 9 READMEs √ó 500 words\n",
      "Sparsity: 83.4%\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for topic modeling\n",
    "repo_names = list(readmes_dict.keys())\n",
    "readme_texts = list(readmes_dict.values())\n",
    "\n",
    "# Preprocess READMEs (reuse the same preprocessing function)\n",
    "readmes_processed = [preprocess_text(text) for text in readme_texts]\n",
    "\n",
    "# Vectorize\n",
    "vectorizer_github = CountVectorizer(\n",
    "    max_features=500,  # Top 500 words\n",
    "    min_df=1,          # Word must appear in at least 1 README\n",
    "    max_df=0.7,        # Ignore words in >70% of READMEs\n",
    "    stop_words='english'  # Remove common English words\n",
    ")\n",
    "\n",
    "doc_term_matrix_github = vectorizer_github.fit_transform(readmes_processed)\n",
    "feature_names_github = vectorizer_github.get_feature_names_out()\n",
    "\n",
    "print(f\"Document-Term Matrix: {doc_term_matrix_github.shape[0]} READMEs √ó {doc_term_matrix_github.shape[1]} words\")\n",
    "print(f\"Sparsity: {(1 - doc_term_matrix_github.nnz / (doc_term_matrix_github.shape[0] * doc_term_matrix_github.shape[1])) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "616460dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA to discover 5 topics in GitHub READMEs...\n",
      "‚úì Training complete!\n",
      "\n",
      "======================================================================\n",
      "DISCOVERED TOPICS IN GITHUB REPOSITORIES\n",
      "======================================================================\n",
      "\n",
      "Topic 1:\n",
      "  Top words: data, csv, cli, output, sh, json, listings, python, profiler, web, project, scraper, raw, report, aqar_fm_listings\n",
      "  Weights: ['37.352', '31.735', '15.120', '15.055', '13.281', '10.462', '10.385', '9.505', '7.699', '7.684', '7.680', '7.645', '6.741', '5.836', '5.810']\n",
      "\n",
      "Topic 2:\n",
      "  Top words: need, pipenv, use, python, voice, command, pip, using, azurestreambot, script, installation, tts, env, dev, package\n",
      "  Weights: ['4.927', '4.589', '2.504', '2.474', '2.468', '2.462', '2.425', '2.149', '2.122', '1.429', '1.386', '1.382', '1.364', '1.363', '1.360']\n",
      "\n",
      "Topic 3:\n",
      "  Top words: files, script, bot, directory, notes, use, moves, discord, installation, uv, tool, functions, extension, daily, 01\n",
      "  Weights: ['14.238', '9.588', '8.626', '6.786', '6.754', '4.913', '4.909', '4.893', '4.890', '4.018', '4.002', '3.990', '3.982', '3.981', '3.981']\n",
      "\n",
      "Topic 4:\n",
      "  Top words: api, otp, vehicles, post, vehicle, login, request, auth, admin, sales, net, register, purchase, update, actions\n",
      "  Weights: ['17.890', '15.107', '8.605', '7.676', '6.768', '6.743', '5.840', '5.826', '5.820', '5.820', '4.879', '4.866', '3.974', '3.970', '3.962']\n",
      "\n",
      "Topic 5:\n",
      "  Top words: cipher, echo, blowfish, bin, pass, cbc, aes, 128, aes128, cfb, ofb, encryption, openssl, password, bf\n",
      "  Weights: ['58.834', '40.145', '33.694', '29.999', '29.986', '28.147', '28.099', '22.539', '19.740', '18.822', '18.815', '17.896', '17.854', '16.962', '16.953']\n",
      "\n",
      "======================================================================\n",
      "REPOSITORIES ORGANIZED BY TOPIC\n",
      "======================================================================\n",
      "\n",
      "üìÅ Topic 1 (2 repositories):\n",
      "  ‚Ä¢ aqar-scraper (99.8%)\n",
      "  ‚Ä¢ CSVProfiler (99.5%)\n",
      "\n",
      "üìÅ Topic 2 (1 repositories):\n",
      "  ‚Ä¢ AzureStreamerBot (98.4%)\n",
      "\n",
      "üìÅ Topic 3 (2 repositories):\n",
      "  ‚Ä¢ Common-Utility-Scripts (99.5%)\n",
      "  ‚Ä¢ DisNote-bot (99.5%)\n",
      "\n",
      "üìÅ Topic 4 (3 repositories):\n",
      "  ‚Ä¢ AzureTTVChat (62.6%)\n",
      "  ‚Ä¢ CarDealerShipAPI (99.7%)\n",
      "  ‚Ä¢ Foley-Mixer (90.0%)\n",
      "\n",
      "üìÅ Topic 5 (1 repositories):\n",
      "  ‚Ä¢ InfoSecProject (99.9%)\n"
     ]
    }
   ],
   "source": [
    "# Train LDA on GitHub READMEs\n",
    "n_topics_github = 5  # Looking for 5 topics: AI, Security, Web, Embedded, Automation\n",
    "\n",
    "lda_github = LatentDirichletAllocation(\n",
    "    n_components=n_topics_github,\n",
    "    random_state=42,\n",
    "    max_iter=20,\n",
    "    learning_method='online'\n",
    ")\n",
    "\n",
    "print(f\"Training LDA to discover {n_topics_github} topics in GitHub READMEs...\")\n",
    "lda_github.fit(doc_term_matrix_github)\n",
    "print(\"‚úì Training complete!\\n\")\n",
    "\n",
    "# Display discovered topics\n",
    "print(\"=\" * 70)\n",
    "print(\"DISCOVERED TOPICS IN GITHUB REPOSITORIES\")\n",
    "print(\"=\" * 70)\n",
    "display_topics(lda_github, feature_names_github, n_top_words=15)\n",
    "\n",
    "# Get topic distribution for each README\n",
    "doc_topic_dist_github = lda_github.transform(doc_term_matrix_github)\n",
    "dominant_topics_github = doc_topic_dist_github.argmax(axis=1)\n",
    "\n",
    "# Create results DataFrame\n",
    "df_github_results = pd.DataFrame({\n",
    "    'Repository': repo_names,\n",
    "    'Dominant Topic': dominant_topics_github + 1,\n",
    "    'Topic Probabilities': [dist for dist in doc_topic_dist_github]\n",
    "})\n",
    "\n",
    "# Display repositories organized by topic\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"REPOSITORIES ORGANIZED BY TOPIC\")\n",
    "print(\"=\" * 70)\n",
    "for topic_id in range(n_topics_github):\n",
    "    topic_repos = df_github_results[df_github_results['Dominant Topic'] == topic_id + 1]\n",
    "    print(f\"\\nüìÅ Topic {topic_id + 1} ({len(topic_repos)} repositories):\")\n",
    "    for idx, row in topic_repos.iterrows():\n",
    "        prob = row['Topic Probabilities'][topic_id]\n",
    "        print(f\"  ‚Ä¢ {row['Repository']} ({prob:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213d4f9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we accomplished**:\n",
    "1. ‚úÖ Loaded unlabeled CVs from a folder\n",
    "2. ‚úÖ Preprocessed the text data\n",
    "3. ‚úÖ Created a document-term matrix\n",
    "4. ‚úÖ Trained an LDA model to discover topics\n",
    "5. ‚úÖ Analyzed which CVs belong to which topic\n",
    "6. ‚úÖ **Automatically organized CVs into folders** based on discovered topics\n",
    "\n",
    "**Key Takeaways**:\n",
    "- **LDA discovers topics automatically** by finding words that co-occur together\n",
    "- **Each document is a mixture of topics** - LDA assigns probabilities\n",
    "- **Topic modeling is unsupervised** - no labels needed!\n",
    "- **Practical application**: Organize unlabeled documents automatically\n",
    "\n",
    "**Next Steps**:\n",
    "- Try different numbers of topics (`n_topics`) and see how results change\n",
    "- Experiment with preprocessing (stemming, stop words removal)\n",
    "- Use topic probabilities to handle CVs that belong to multiple topics\n",
    "- Visualize topics using tools like pyLDAvis\n",
    "\n",
    "**References**:\n",
    "- [Scikit-learn LDA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n",
    "- [Topic modeling visualization guide](https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8790015",
   "metadata": {},
   "source": [
    "\n",
    "## Module 1 Synthesis: The Complete Pipeline\n",
    "\n",
    "Congratulations! You've completed **Module 1: Text Analysis with Statistical NLP**. Let's reflect on the journey and see how all the pieces fit together.\n",
    "\n",
    "### The Circular Learning Experience\n",
    "\n",
    "Remember the question chain we started with? Let's trace how we answered each question and built a complete NLP pipeline:\n",
    "\n",
    "1. **\"What is NLP?\"** ‚Üí We learned that NLP bridges computers and human language, with applications in understanding and generation.\n",
    "\n",
    "2. **\"How do we extract patterns from text?\"** ‚Üí We used **Regular Expressions** to find, match, and manipulate text patterns‚Äîessential for preprocessing.\n",
    "\n",
    "3. **\"How do we understand our data?\"** ‚Üí We performed **Exploratory Data Analysis (EDA)** on corpora to assess data quality, vocabulary characteristics, and preprocessing needs.\n",
    "\n",
    "4. **\"How do we prepare text for ML?\"** ‚Üí We applied **Preprocessing** techniques (cleaning, normalization, tokenization, stemming) to transform raw text into clean tokens.\n",
    "\n",
    "5. **\"How do we convert text to numbers?\"** ‚Üí We used **Vectorization** (BoW, TF-IDF) to convert text into numerical features that ML models can process.\n",
    "\n",
    "6. **\"How do we build classifiers?\"** ‚Üí We built **Text Classification** models (like sentiment analysis) using vectorized features and supervised learning.\n",
    "\n",
    "7. **\"How do we search documents?\"** ‚Üí We implemented **Information Retrieval** systems using TF-IDF and cosine similarity to find relevant documents.\n",
    "\n",
    "8. **\"How do we discover topics?\"** ‚Üí We applied **Topic Modeling** (LDA) to automatically organize unlabeled documents by discovering hidden topics.\n",
    "\n",
    "### The Complete NLP Pipeline\n",
    "\n",
    "Throughout this module, you've learned to build a complete NLP pipeline:\n",
    "\n",
    "```\n",
    "Raw Text\n",
    "    ‚Üì\n",
    "[Regex: Pattern Extraction]\n",
    "    ‚Üì\n",
    "[Corpus & EDA: Understanding Data]\n",
    "    ‚Üì\n",
    "[Preprocessing: Cleaning & Normalization]\n",
    "    ‚Üì\n",
    "[Vectorization: Text ‚Üí Numbers]\n",
    "    ‚Üì\n",
    "[Modeling: Classification / IR / Topic Modeling]\n",
    "    ‚Üì\n",
    "Actionable Insights\n",
    "```\n",
    "\n",
    "### Key Skills You've Acquired\n",
    "\n",
    "By completing this module, you can now:\n",
    "\n",
    "‚úÖ **Build supervised ML text classification pipelines**\n",
    "- Preprocess Arabic and English text\n",
    "- Vectorize text using BoW and TF-IDF\n",
    "- Train and evaluate classifiers\n",
    "- Interpret model results\n",
    "\n",
    "‚úÖ **Apply keyword-based information retrieval**\n",
    "- Implement TF-IDF-based search engines\n",
    "- Measure document similarity using cosine similarity\n",
    "- Rank and retrieve relevant documents\n",
    "\n",
    "‚úÖ **Apply unsupervised ML for document organization**\n",
    "- Discover hidden topics using LDA\n",
    "- Organize unlabeled documents automatically\n",
    "- Interpret topic modeling results\n",
    "\n",
    "### The Foundation for What's Next\n",
    "\n",
    "This module focused on **statistical NLP**‚Äîtraditional methods that work well for many tasks. In **Module 2**, you'll learn about **Deep Learning approaches** (embeddings, transformers) that build on these foundations to achieve even better performance.\n",
    "\n",
    "**What you learned here is still valuable:**\n",
    "- Preprocessing techniques apply to both statistical and deep learning methods\n",
    "- Understanding vectorization helps you understand embeddings\n",
    "- EDA is always the first step, regardless of the approach\n",
    "- The pipeline structure (preprocess ‚Üí vectorize ‚Üí model) remains the same\n",
    "\n",
    "### Reflection Questions\n",
    "\n",
    "Before moving to Module 2, consider:\n",
    "\n",
    "1. **When would you use statistical NLP vs. deep learning?**\n",
    "   - Statistical NLP: Fast, interpretable, works with small data\n",
    "   - Deep Learning: Better accuracy, requires more data and computation\n",
    "\n",
    "2. **What preprocessing steps are most important?**\n",
    "   - Depends on your data and task, but EDA always guides the decision\n",
    "\n",
    "3. **How does TF-IDF differ from BoW?**\n",
    "   - BoW: Simple word counts\n",
    "   - TF-IDF: Weighted counts that emphasize distinctive words\n",
    "\n",
    "4. **When would you use topic modeling vs. classification?**\n",
    "   - Classification: When you have labels and want to predict categories\n",
    "   - Topic Modeling: When you have no labels and want to discover structure\n",
    "\n",
    "### The Journey Continues\n",
    "\n",
    "You've built a solid foundation in statistical NLP. The concepts you've learned‚Äîpreprocessing, vectorization, classification, retrieval, and topic modeling‚Äîare the building blocks for more advanced techniques.\n",
    "\n",
    "**Next Module Preview:**\n",
    "- **Module 2** introduces **Deep Learning for NLP**:\n",
    "  - Tokenization with modern tools (WordPiece, BPE)\n",
    "  - Word embeddings (Word2Vec, GloVe, contextual embeddings)\n",
    "  - Transformers and BERT\n",
    "  - Fine-tuning pre-trained models\n",
    "\n",
    "The journey from statistical NLP to deep learning is a natural progression‚Äîyou'll see how embeddings generalize vectorization, how transformers improve on traditional methods, and how pre-trained models leverage the foundations you've built.\n",
    "\n",
    "---\n",
    "\n",
    "**Module 1 Complete! üéâ**\n",
    "\n",
    "You now have the skills to work with text data using statistical methods. You understand the complete pipeline from raw text to actionable insights, and you're ready to explore the power of deep learning in Module 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
